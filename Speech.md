# Great Deep Learning Tutorials & Resources for Speech Processing
A Great Collection of Deep Learning Tutorials and Repositories for Speech Processing

## General (Spoken Language Processing (Speech Processing)):  
- [Audio Classification](https://towardsdatascience.com/audio-classification-using-fastai-and-on-the-fly-frequency-transforms-4dbe1b540f89) [_Great_]  
- [Building a Dead Simple Word Recognition Engine Using Convnet](https://blog.manash.me/building-a-dead-simple-word-recognition-engine-using-convnet-in-keras-25e72c19c12b)  
- [Identifying the Genre of a Song with Neural Networks](https://medium.com/@navdeepsingh_2336/identifying-the-genre-of-a-song-with-neural-networks-851db89c42f0)  
- [Modelling audio signal using visual features](https://raghavgoyal14.github.io/2018/04/12/audio-via-vid-features.html)  
- [ESC-50: Dataset for Environmental Sound Classification](https://github.com/karolpiczak/ESC-50)  
- [Kaldi Speech Recognition Toolkit](https://github.com/kaldi-asr/kaldi)  
- [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi)  
- [SpeechBrain - PyTorch-based Speech Toolkit](https://speechbrain.github.io/)  
- [How to start with Kaldi and Speech Recognition](https://towardsdatascience.com/how-to-start-with-kaldi-and-speech-recognition-a9b7670ffff6)  
- [A 2019 Guide to Speech Synthesis with Deep Learning](https://heartbeat.fritz.ai/a-2019-guide-to-speech-synthesis-with-deep-learning-630afcafb9dd)  
- [A 2019 Guide for Automatic Speech Recognition](https://heartbeat.fritz.ai/a-2019-guide-for-automatic-speech-recognition-f1e1129a141c)  
- [PyKaldi](https://github.com/pykaldi/pykaldi)  
- [WaveNet vocoder](https://github.com/r9y9/wavenet_vocoder)  
- [nnAudio - audio processing toolbox using PyTorch](https://github.com/KinWaiCheuk/nnAudio)  
- [Athena - open-source implementation of end-to-end speech processing engine](https://github.com/athena-team/athena/tree/simclr)   
- [Pydub - manipulate audio](https://github.com/jiaaro/pydub)  
- [pyAcoustics - analyzing acoustics from audio files](https://github.com/timmahrt/pyAcoustics)  
- [ESPnet: end-to-end speech processing toolkit](https://github.com/espnet/espnet)  
- [WeNet](https://github.com/wenet-e2e/wenet) [Great]  
- [WeNet Android App](https://github.com/wenet-e2e/wenet/tree/main/runtime/device/android/wenet)   
- [K2: FSA/FST algorithms, differentiable, with PyTorch compatibility](https://github.com/k2-fsa/k2)  
- [Microsoft NeuralSpeech](https://github.com/microsoft/NeuralSpeech)  
- [Great Speech Tutorials: alphacephei](https://alphacephei.com/nsh/)  
- [NeMo - toolkit for Conversational AI](https://github.com/NVIDIA/NeMo) [_Excellent_]  

## Text to Speech (TTS):
- [Glow-TTS](https://github.com/jaywalnut310/glow-tts)  
- [ForwardTacotron](https://github.com/as-ideas/ForwardTacotron)  
- [WaveRNN Vocoder + TTS](https://github.com/fatchord/WaveRNN)  
- [Deep Voice 3 PyTorch](https://github.com/r9y9/deepvoice3_pytorch)  
- [MelGAN - TTS - version1](https://github.com/descriptinc/melgan-neurips)  
- [MelGAN - TTS - version2](https://github.com/seungwonpark/melgan)  
- [FastSpeech - TTS - version1](https://github.com/xcmyz/FastSpeech)  
- [FastSpeech - TTS - version2](https://github.com/ming024/FastSpeech2)  
- [Speedy Speech](https://github.com/janvainer/speedyspeech)  
- [Mozilla - TTS](https://github.com/mozilla/TTS)  
- [YourTTS: Zero-Shot Multi-Speaker TTS](https://github.com/edresson/yourtts)  
- [YourTTS: Zero-Shot Multi-Speaker Text Synthesis and Voice Conversion](https://coqui.ai/blog/tts/yourtts-zero-shot-text-synthesis-low-resource-languages/)  
- [Nix-TTS](https://github.com/rendchevi/nix-tts)  
- [TorToiSe](https://github.com/neonbjb/tortoise-tts)  
- [Amazon TTS Group's Research](https://www.amazon.science/blog/amazon-text-to-speech-groups-research-at-icassp-2022)  
- [NVIDIA RADTTS](https://github.com/NVIDIA/radtts)  
- [CanTTS: a single-speaker Cantonese speech dataset for TTS](https://github.com/parami-ai/CanTTS)  
- [Lightning Fast Speech2](https://github.com/MiniXC/LightningFastSpeech2)  
- [ProDiff: Progressive Fast Diffusion Model For High-Quality TTS](https://github.com/Rongjiehuang/ProDiff)  
- [TF light model (Mozilla tacotron2)](https://github.com/mozilla/TTS/blob/master/notebooks/DDC_TTS_and_MultiBand_MelGAN_TFLite_Example.ipynb)  
- [Lightweight end-to-end TTS](https://github.com/MasayaKawamura/MB-iSTFT-VITS)  
- [SiFiGAN](https://github.com/chomeyama/SiFiGAN)  
- [Neon TTS Plugin Coqui](https://huggingface.co/spaces/neongeckocom/neon-tts-plugin-coqui)  
- [VocBench: A Neural Vocoder Benchmark for Speech Synthesis](https://github.com/facebookresearch/vocoder-benchmark)  
- [MQTTS: Quantized Approach for Text to Speech Synthesis](https://github.com/b04901014/MQTTS)  
- [VITS Fast Fine-tuning: fast speaker adaptation TTS](https://github.com/Plachtaa/VITS-fast-fine-tuning)  
- [Larynx: A fast, local neural TTS](https://github.com/rhasspy/larynx2)  
- [BigVGAN: A Universal Neural Vocoder with Large-Scale Training](https://github.com/NVIDIA/BigVGAN)  
- [Bark: Text-Prompted Generative Audio Model](https://github.com/suno-ai/bark)  
- [FaceBook Massively Multilingual Speech (MMS)](https://github.com/facebookresearch/fairseq/tree/main/examples/mms)
- [AudioLDM2: unified framework for text-to-audio generation](https://github.com/haoheliu/AudioLDM2)  

## Automatic Speech Recognition (ASR) & Speech to Text (STT):
- [OpenSpeech](https://github.com/openspeech-team/openspeech) [Great]   
- [wav2letter++](https://github.com/facebookresearch/wav2letter)  
- [End-to-End ASR - PyTorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch)  
- [NeuralSP](https://github.com/hirofumi0810/neural_sp)  
- [Silero Speech-To-Text Models - PyTorch Hub](https://pytorch.org/hub/snakers4_silero-models_stt/)  
- [Silero Models - GitHub](https://github.com/snakers4/silero-models)  
- [Hugging Faceâ€™s Wav2Vec2 & its First ASR Model](https://www.analyticsvidhya.com/blog/2021/02/hugging-face-introduces-the-first-automatic-speech-recognition-model-wav2vec2/)  
- [Hugging Face - wav2vec2](https://huggingface.co/transformers/model_doc/wav2vec2.html)  
- [PyTorch Wav2Vec](https://github.com/pytorch/fairseq/tree/master/examples/wav2vec/unsupervised)   
- [Self-training and pre-training, understanding the wav2vec series](https://maelfabien.github.io/machinelearning/wav2vec/#)   
- [Conformer](https://github.com/sooftware/conformer)  
- [Emformer: RNNT Model](https://github.com/pytorch/audio/tree/main/examples/asr/emformer_rnnt)  
- [Emformer Paper](https://arxiv.org/abs/2010.10759)  
- [Nextformer](https://github.com/tuanio/nextformer)  
- [Keras based Training a CTC-based model for ASR](https://keras.io/examples/audio/ctc_asr/)   
- [alphacephei: citrinet](https://alphacephei.com/nsh/2021/04/23/citrinet.html)  
- [Coqui-ai STT](https://github.com/coqui-ai/STT)  
- [vosk Framework](https://alphacephei.com/vosk/)  
- [vosk Framework GitHub](https://github.com/alphacep/vosk-api)  
- [fairseq](https://github.com/pytorch/fairseq)   
- [TensorFlowASR](https://github.com/TensorSpeech/TensorFlowASR) [Good]    
- [Assembly AI ASR api](https://www.assemblyai.com/)  
- [Assembly AI: Building an End-to-End Speech Recognition Model in PyTorch](https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch/) [Great]  
- [BigSSL: Large-Scale Semi-Supervised Learning for Automatic Speech Recognition](https://arxiv.org/pdf/2109.13226.pdf)  
- [Tencent AI Lab: 3M-ASR](https://github.com/tencent-ailab/3m-asr)  
- [wav2seq](https://github.com/asappresearch/wav2seq)  
- [WavPrompt: speech understanding that leveraging the few-shot learning](https://github.com/Hertin/WavPrompt)  
- [Recent Advances in End-to-End Automatic Speech Recognition](https://www.nowpublishers.com/article/Details/SIP-2021-0050) [Interesting Survey]  
- [SpeechT5](https://github.com/microsoft/SpeechT5) [Interesting]  
- [TransFusion: Transcribing Speech with Multinomial Diffusion](https://github.com/RF5/transfusion-asr)  
- [Alibaba FunASR](https://github.com/alibaba-damo-academy/FunASR)  
- [Openai whisper ASR Model](https://github.com/openai/whisper) [Interesting]  
- [Openai whisper ASR Model Blog](https://openai.com/blog/whisper/)  
- [Explanation of OpenAI whisper ASR Model](https://www.linkedin.com/posts/aladdin-persson-a95384153_a-couple-of-months-ago-openai-released-its-activity-7009429017552375808-6zCI/?utm_source=share&utm_medium=member_android)  
- [High-performance inference of Whisper ASR Model](https://github.com/Const-me/Whisper)  
- [Google Universal Speech Model (USM)](https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html?m=1)  
- [FaceBook Massively Multilingual Speech (MMS)](https://github.com/facebookresearch/fairseq/tree/main/examples/mms)
- [SeamlessM4T: Github](https://github.com/facebookresearch/seamless_communication)
- [SeamlessM4T: Meta AI Blog](https://ai.meta.com/blog/seamless-m4t/)
- [SeamlessM4T: Paper](https://dl.fbaipublicfiles.com/seamless/seamless_m4t_paper.pdf)  
- [SeamlessM4T: Demo](https://seamless.metademolab.com/demo)  
- [SeamlessM4T: HuggingFace Demo](https://huggingface.co/spaces/facebook/seamless_m4t)
- [SeamlessM4T v2](https://www.linkedin.com/posts/aiatmeta_today-were-sharing-the-next-milestone-in-activity-7136104235166830592-7ROv?utm_source=share&utm_medium=member_desktop)  
- [HuggingFace Open ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)  

### Speech Language Modeling:
- [An Empirical Study of Language Model Integration for Transducer based Speech Recognition](https://arxiv.org/pdf/2203.16776.pdf)  
- [AudioPaLM](https://google-research.github.io/seanet/audiopalm/examples/)  

### Persian ASR Repos:
- [wav2vec2-fa](https://github.com/Hamtech-ai/wav2vec2-fa)  
- [Shenasa-ai Speech2Text](https://github.com/shenasa-ai/speech2text)  
- [SOTA Persian ASR on Common Voice](https://paperswithcode.com/sota/speech-recognition-on-common-voice-persian)  
- [Wav2Vec2 Large-xlsr Persian](https://huggingface.co/m3hrdadfi/wav2vec2-large-xlsr-persian)  
- [Wav2Vec2 Large-xlsr Persian (v3)](https://huggingface.co/m3hrdadfi/wav2vec2-large-xlsr-persian-v3)  
- [~ 200 Hours Persian ASR Data Set of Shenasa Company](https://github.com/shenasa-ai/speech2text)  

### Great Resources for Persian ASR Normalization:
- [Wav2Vec2 Large XLSR Persian v3](https://huggingface.co/m3hrdadfi/wav2vec2-large-xlsr-persian-v3)  
- [num2fawords: Convert a number into Persian word form](https://github.com/5j9/num2fawords)  
- [Parsivar: A Language Processing Toolkit for Persian](https://github.com/ICTRC/Parsivar)  
- [num2words](https://github.com/savoirfairelinux/num2words)  

### Persian based Raw Text Data Sets for LM Training:
- [naab-raw](https://huggingface.co/datasets/SLPL/naab-raw)  

### `Adapters` Method instead of fine-tuning for Large-Scale ASR models:
- [NeMo Adapters Tutorial](https://github.com/NVIDIA/NeMo/blob/main/tutorials/02_NeMo_Adapters.ipynb)  
- [Paper: Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751.pdf)  
- [Paper: Efficient Adapter Transfer of Self-Supervised Speech Models for Automatic Speech Recognition](https://arxiv.org/pdf/2202.03218.pdf)  
- [Paper: Exploiting Adapters for Cross-lingual Low-resource Speech Recognition](https://arxiv.org/pdf/2105.11905.pdf)  
- [Paper: Tiny-Attention Adapter: Contexts Are More Important Than the Number of Parameters](https://arxiv.org/abs/2211.01979)  

## Diffusion based Methods:
- [Full-band General Audio Synthesis With Score-based Diffusion](https://diffusionaudiosynthesis.github.io/)  

## Audio Generation:
- [Unconditional Audio Generation Benchmark](https://github.com/gzhu06/Unconditional-Audio-Generation-Benchmark)
- [Realtime Voice Changer](https://github.com/w-okada/voice-changer/tree/master)  

## Speech Translation:
- [Facebook XLS-R-2B-22-16 Demo](https://huggingface.co/spaces/facebook/XLS-R-2B-22-16)  
- [Facebook wav2vec2-xls-r-2b-22-to-16](https://huggingface.co/facebook/wav2vec2-xls-r-2b-22-to-16)  
- [Facebook XLS-R-2B-22-16 app code](https://huggingface.co/spaces/facebook/XLS-R-2B-22-16/blob/main/app.py)  

## G2P (Grapheme2Phoneme):
- [English Grapheme To Phoneme (G2P) Conversion](https://github.com/Kyubyong/g2p)   
- [Phonemizer: Simple text to phones converter for multiple languages](https://github.com/bootphon/phonemizer)   
- [Epitran: tool for transcribing orthographic text as IPA](https://github.com/dmort27/epitran)   
- [PersianG2P](https://github.com/PasaOpasen/PersianG2P)  
- [Persian_G2P - link2](https://github.com/AzamRabiee/Persian_G2P)   
- [Persian Attention Based G2P](https://github.com/hajix/G2P#attention-based-grapheme-to-phoneme)  
- [Tihu Dictionary for Persian Language](https://github.com/tihu-nlp/tihudict)  
- [CharsiuG2P: Multilingual G2P in over 100 languages](https://github.com/lingjzhu/CharsiuG2P)  
- [Transphone: zero-shot learning based grapheme-to-phoneme model for 8k languages](https://github.com/xinjli/transphone)  

## Fundamental Notes in Speech Processing:
- [Deep Learning for Audio (DLA)](https://github.com/markovka17/dla) [**Great Course**]  
- [MFCC Tutorial](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/)  
- [Sequence Modeling With CTC](https://distill.pub/2017/ctc/)  
- [Explanation of Connectionist Temporal Classification](https://sid2697.github.io/Blog_Sid/algorithm/2019/10/19/CTC-Loss.html)  
- [D2L Beam Search](https://d2l.ai/chapter_recurrent-modern/beam-search.html)  
- [D2L Attention Mechanisms](https://d2l.ai/chapter_attention-mechanisms/index.html)  
- [Introduction to Speech Processing](https://wiki.aalto.fi/display/ITSP/Introduction+to+Speech+Processing) [Good]  
- [Audio Signal Proessing for Machine Learning](https://github.com/musikalkemist/AudioSignalProcessingForML)  
- [Deep Learning For Audio With Python](https://github.com/musikalkemist/DeepLearningForAudioWithPython)  
- [Deep learning (audio) application: From design to deployment](https://github.com/musikalkemist/Deep-Learning-Audio-Application-From-Design-to-Deployment)  
- [ASR 2022](https://github.com/besacier/ASR2022)  

## Great Kaldi Tutorials:
- [Kaldi Install for Dummies](https://www.assemblyai.com/blog/kaldi-install-for-dummies/)  
- [Kaldi Speech Recognition for Beginners a Simple Tutorial](https://www.assemblyai.com/blog/kaldi-speech-recognition-for-beginners-a-simple-tutorial/)  
- [Tutorial on Kaldi for Brandeis ASR course](https://github.com/keighrim/kaldi-yesno-tutorial)  

## ASR Error Correction:
- [FastCorrect](https://arxiv.org/pdf/2105.03842.pdf)  

## Source Separation:
- [Deezer source separation library](https://github.com/deezer/spleeter) [Great]   
- [Music Source Separation Challenge](https://github.com/haoheliu/2021-ISMIR-MSS-Challenge-CWS-PResUNet)  

## Sound & Audio Classification:
- [Soxan: Wav2Vec2 for speech recognition](https://github.com/m3hrdadfi/soxan)  

## Voice Activity Detection (VAD) & Speech Activity Detection (SAD):
- [Silero VAD](https://github.com/snakers4/silero-vad)  
- [Voice Activity Detection: Identifying whether someone is speaking or not](https://maelfabien.github.io/project/Speech_proj/#) [**Great**]    
- [py-webrtcvad: Python WebRTC Voice Activity Detector (VAD)](https://github.com/wiseman/py-webrtcvad) [also, it seems that it can segment audio files]   
- [Pyannote Audio](https://github.com/pyannote/pyannote-audio)  
- [Remsi: Remove silence from video files via ffmpeg](https://github.com/bambax/Remsi)  

## Audio Segmentation:  
- [inaSpeechSegmenter](https://github.com/ina-foss/inaSpeechSegmenter)  

## Extract & Remove Vocals from Song in Audio Files:
- [Moises: Remove or isolate vocals and instruments in any song](https://moises.ai/)  

## Audio Summarization:  
- [Audio Summarization API](https://github.com/ifrankandrade/api)  

## Spoken Language Recognition:
- [Spoken Language Recognition based on Kaldi](https://github.com/igorsitdikov/lid_kaldi)  

## Keyword Spotting & Speech Command Recognition:
- [PyTorch based toolkit for speech command recognition](https://github.com/idiap/sparch)  
- [Multilingual Few-Shot Keyword Spotting in PyTorch](https://github.com/alefiury/multilingual_kws_pytorch)  

## Active Learning in ASR:
- [Active learning in speech recognition](https://alphacephei.com/nsh/2021/07/13/active-learning.html)   

## Audio Pretraining, Representation Learning, and Self-Supervised Pretraining:
- [CLAP: Contrastive Language-Audio Pretraining](https://github.com/LAION-AI/CLAP)  

## Audio Augmentation:
- [Audiomentations: Audio Data Augmentation](https://github.com/iver56/audiomentations)  

## Speech Emotion Recognition:
- [Speech Emotion Recognition via wav2vec2](https://github.com/audeering/w2v2-how-to)  
- [Speech Emotions Recognition with Convolutional Neural Networks](https://www.analyticsvidhya.com/blog/2021/07/speech-emotions-recognition-with-convolutional-neural-networks/)  
- [ShEMO Data Set](https://github.com/mansourehk/ShEMO)  

## Annotation Tools:
- [audino: open source audio annotation tool](https://github.com/midas-research/audino)   
- [ASR Corpus Creator](https://github.com/egorsmkv/asr-corpus-creator)  
- [Speech Editing Toolkit](https://github.com/Zain-Jiang/Speech-Editing-Toolkit)  

## Audio Compression:
- [EnCodec: High Fidelity Neural Audio Compression](https://github.com/facebookresearch/encodec)  
- [AI Powered Audio Compression Technique](https://ai.facebook.com/blog/ai-powered-audio-compression-technique/)  
## Audio Variational Autoencoder (VAE):
- [AudioVAE: VAE implementation for audio data](https://github.com/nikuson/AudioVAE)  

## Some ASR & Speech Datasets:
- [Peoples Speech](https://mlcommons.org/en/peoples-speech/)  
- [Multilingual Spoken Words](https://mlcommons.org/en/multilingual-spoken-words/)  
- [PodcastMix: A dataset for separating music and speech in podcasts](https://github.com/MTG/Podcastmix)  
- [Quran Speech to Text Dataset](https://www.openslr.org/132/)  
- [WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset](https://github.com/xinhaomei/wavcaps)  

# Interesting Ideas about Startups with ASR:
It is interesting how quickly people implement ideas. Like the one of `podcast transcript` with Whisper. Here is a selection:  
- [podscript](https://podscript.ai/)  
- [podtext](https://podtext.ai/)  
- [podscription](https://podscription.app/)  
- [podsearch](https://podsearch.page/)  
- [Some Discussion Notes about above links](https://news.ycombinator.com/item?id=34727695)  

# Other:
- [Neural Target Speech Extraction (TSE)](https://butspeechfit.github.io/tse_tutorial/)  
- [Audio Self-supervised Learning: A Survey](https://arxiv.org/abs/2203.01205)  
- [AI Audio Startups](https://github.com/csteinmetz1/ai-audio-startups)  
- [Facestar: High quality audio-visual recordings of human conversational speech](https://github.com/facebookresearch/facestar)  
- [Fast Infinite Waveform Music Generation](https://github.com/marcoppasini/musika)  
- [Nvidia Speech AI Summit 2022](https://www.nvidia.com/en-us/events/speech-ai-summit/)  
- [Poly AI](https://poly.ai/) [Interesting Company]  
- [uberduck: Open Source Voice AI Community](https://uberduck.ai/)  

# Great Deep Learning Tutorials & Resources for Speech Processing
A Great Collection of Deep Learning Tutorials and Repositories for Speech Processing

## General (Spoken Language Processing (Speech Processing)):  
- [Audio Classification](https://towardsdatascience.com/audio-classification-using-fastai-and-on-the-fly-frequency-transforms-4dbe1b540f89) [_Great_]  
- [Building a Dead Simple Word Recognition Engine Using Convnet](https://blog.manash.me/building-a-dead-simple-word-recognition-engine-using-convnet-in-keras-25e72c19c12b)  
- [Identifying the Genre of a Song with Neural Networks](https://medium.com/@navdeepsingh_2336/identifying-the-genre-of-a-song-with-neural-networks-851db89c42f0)  
- [Modelling audio signal using visual features](https://raghavgoyal14.github.io/2018/04/12/audio-via-vid-features.html)  
- [ESC-50: Dataset for Environmental Sound Classification](https://github.com/karolpiczak/ESC-50)  
- [Kaldi Speech Recognition Toolkit](https://github.com/kaldi-asr/kaldi)  
- [PyTorch-Kaldi](https://github.com/mravanelli/pytorch-kaldi)  
- [SpeechBrain - PyTorch-based Speech Toolkit](https://speechbrain.github.io/)  
- [How to start with Kaldi and Speech Recognition](https://towardsdatascience.com/how-to-start-with-kaldi-and-speech-recognition-a9b7670ffff6)  
- [A 2019 Guide to Speech Synthesis with Deep Learning](https://heartbeat.fritz.ai/a-2019-guide-to-speech-synthesis-with-deep-learning-630afcafb9dd)  
- [A 2019 Guide for Automatic Speech Recognition](https://heartbeat.fritz.ai/a-2019-guide-for-automatic-speech-recognition-f1e1129a141c)  
- [PyKaldi](https://github.com/pykaldi/pykaldi)  
- [WaveNet vocoder](https://github.com/r9y9/wavenet_vocoder)  
- [nnAudio - audio processing toolbox using PyTorch](https://github.com/KinWaiCheuk/nnAudio)  
- [Athena - open-source implementation of end-to-end speech processing engine](https://github.com/athena-team/athena/tree/simclr)   
- [Pydub - manipulate audio](https://github.com/jiaaro/pydub)  
- [pyAcoustics - analyzing acoustics from audio files](https://github.com/timmahrt/pyAcoustics)  
- [ESPnet: end-to-end speech processing toolkit](https://github.com/espnet/espnet)  
- [WeNet](https://github.com/wenet-e2e/wenet) [Great]  
- [WeNet Android App](https://github.com/wenet-e2e/wenet/tree/main/runtime/device/android/wenet)   
- [K2: FSA/FST algorithms, differentiable, with PyTorch compatibility](https://github.com/k2-fsa/k2)  
- [Microsoft NeuralSpeech](https://github.com/microsoft/NeuralSpeech)  
- [Great Speech Tutorials: alphacephei](https://alphacephei.com/nsh/)
- [AssemblyAI Lead Speech AI Models](https://www.assemblyai.com/products?utm_source=alphasignal&utm_medium=newsletter_sponsor&utm_campaign=spotlight&utm_content=82624)  
- [open-mmlab Amphion: An Open-Source Audio, Music, and Speech Generation Toolkit](https://github.com/open-mmlab/Amphion)
- [HuggingFace Speech-to-Speech Library](https://github.com/huggingface/speech-to-speech) [**Great**]  
- [HuggingFace Speech-to-Speech Library News](https://www.linkedin.com/posts/andresmarafioti_introducing-hugging-faces-speech-to-speech-activity-7231548059388723201-1wIe?utm_source=share&utm_medium=member_desktop)  
- [NeMo - toolkit for Conversational AI](https://github.com/NVIDIA/NeMo) [_Excellent_]  

## Text to Speech (TTS):
- [Glow-TTS](https://github.com/jaywalnut310/glow-tts)  
- [ForwardTacotron](https://github.com/as-ideas/ForwardTacotron)  
- [WaveRNN Vocoder + TTS](https://github.com/fatchord/WaveRNN)  
- [Deep Voice 3 PyTorch](https://github.com/r9y9/deepvoice3_pytorch)  
- [MelGAN - TTS - version1](https://github.com/descriptinc/melgan-neurips)  
- [MelGAN - TTS - version2](https://github.com/seungwonpark/melgan)  
- [FastSpeech - TTS - version1](https://github.com/xcmyz/FastSpeech)  
- [FastSpeech - TTS - version2](https://github.com/ming024/FastSpeech2)  
- [Speedy Speech](https://github.com/janvainer/speedyspeech)  
- [Mozilla - TTS](https://github.com/mozilla/TTS)  
- [YourTTS: Zero-Shot Multi-Speaker TTS](https://github.com/edresson/yourtts)  
- [YourTTS: Zero-Shot Multi-Speaker Text Synthesis and Voice Conversion](https://coqui.ai/blog/tts/yourtts-zero-shot-text-synthesis-low-resource-languages/)  
- [Nix-TTS](https://github.com/rendchevi/nix-tts)  
- [TorToiSe](https://github.com/neonbjb/tortoise-tts)  
- [Amazon TTS Group's Research](https://www.amazon.science/blog/amazon-text-to-speech-groups-research-at-icassp-2022)  
- [NVIDIA RADTTS](https://github.com/NVIDIA/radtts)  
- [CanTTS: a single-speaker Cantonese speech dataset for TTS](https://github.com/parami-ai/CanTTS)  
- [Lightning Fast Speech2](https://github.com/MiniXC/LightningFastSpeech2)  
- [ProDiff: Progressive Fast Diffusion Model For High-Quality TTS](https://github.com/Rongjiehuang/ProDiff)  
- [TF light model (Mozilla tacotron2)](https://github.com/mozilla/TTS/blob/master/notebooks/DDC_TTS_and_MultiBand_MelGAN_TFLite_Example.ipynb)  
- [Lightweight end-to-end TTS](https://github.com/MasayaKawamura/MB-iSTFT-VITS)  
- [SiFiGAN](https://github.com/chomeyama/SiFiGAN)  
- [Neon TTS Plugin Coqui](https://huggingface.co/spaces/neongeckocom/neon-tts-plugin-coqui)  
- [VocBench: A Neural Vocoder Benchmark for Speech Synthesis](https://github.com/facebookresearch/vocoder-benchmark)  
- [MQTTS: Quantized Approach for Text to Speech Synthesis](https://github.com/b04901014/MQTTS)  
- [VITS Fast Fine-tuning: fast speaker adaptation TTS](https://github.com/Plachtaa/VITS-fast-fine-tuning)  
- [Larynx: A fast, local neural TTS](https://github.com/rhasspy/larynx2)  
- [BigVGAN: A Universal Neural Vocoder with Large-Scale Training](https://github.com/NVIDIA/BigVGAN)  
- [Bark: Text-Prompted Generative Audio Model](https://github.com/suno-ai/bark)  
- [FaceBook Massively Multilingual Speech (MMS)](https://github.com/facebookresearch/fairseq/tree/main/examples/mms)
- [AudioLDM2: unified framework for text-to-audio generation](https://github.com/haoheliu/AudioLDM2)
- [MetaVoice-1B: a 1.2B parameter base model trained on 100K hours of speech for TTS](https://github.com/metavoiceio/metavoice-src)
- [Parler TTS](https://www.linkedin.com/posts/yoach-lacombe_introducing-data-speech-a-fully-open-source-activity-7183864961905872896-1EBe?utm_source=share&utm_medium=member_desktop)
- [IMS-Toucan TTS: the first TTS System in over 7000 languages](https://github.com/DigitalPhonetics/IMS-Toucan)
- [E2 TTS](https://www.linkedin.com/posts/naoyuki-kanda-16a00511b_e2-tts-microsoft-research-activity-7211927392804593665-ATUA?utm_source=share&utm_medium=member_android)
- [Mars5 TTS](https://www.linkedin.com/posts/vaibhavs10_mars5-tts-open-source-text-to-speech-with-activity-7211102752305819650-RJwf?utm_source=share&utm_medium=member_desktop)
- [Nvidia NeMo T5-TTS Model](https://www.linkedin.com/posts/subhankar-19_addressing-hallucinations-in-speech-synthesis-activity-7214352391993765889-EGVu?utm_source=share&utm_medium=member_desktop)
- [Parler-TTS: fully open-source high-quality TTS](https://huggingface.co/collections/parler-tts/parler-tts-fully-open-source-high-quality-tts-66164ad285ba03e8ffde214c)
- [Fish Speech TTS Models](https://github.com/fishaudio/fish-speech)  
- [Fish Speech V1.4: a leading text-to-speech (TTS) model](https://huggingface.co/fishaudio/fish-speech-1.4)
- [FireRedTTS: A Foundation Text-To-Speech Framework for Industry-Level Generative Speech Applications](https://github.com/FireRedTeam/FireRedTTS)
- [XTTS-v2](https://www.linkedin.com/posts/josh-r-meyer_its-been-almost-a-year-since-we-released-activity-7254933062004666368-FL6T?utm_source=share&utm_medium=member_desktop)
- [Smoll TTS Models](https://www.linkedin.com/posts/vaibhavs10_smol-tts-models-are-here-outetts-01-350m-activity-7259247217750728704-OkZM?utm_source=share&utm_medium=member_desktop)  

## Automatic Speech Recognition (ASR) & Speech to Text (STT):
- [OpenSpeech](https://github.com/openspeech-team/openspeech) [Great]   
- [wav2letter++](https://github.com/facebookresearch/wav2letter)  
- [End-to-End ASR - PyTorch](https://github.com/Alexander-H-Liu/End-to-end-ASR-Pytorch)  
- [NeuralSP](https://github.com/hirofumi0810/neural_sp)  
- [Silero Speech-To-Text Models - PyTorch Hub](https://pytorch.org/hub/snakers4_silero-models_stt/)  
- [Silero Models - GitHub](https://github.com/snakers4/silero-models)  
- [Hugging Faceâ€™s Wav2Vec2 & its First ASR Model](https://www.analyticsvidhya.com/blog/2021/02/hugging-face-introduces-the-first-automatic-speech-recognition-model-wav2vec2/)  
- [Hugging Face - wav2vec2](https://huggingface.co/transformers/model_doc/wav2vec2.html)  
- [PyTorch Wav2Vec](https://github.com/pytorch/fairseq/tree/master/examples/wav2vec/unsupervised)   
- [Self-training and pre-training, understanding the wav2vec series](https://maelfabien.github.io/machinelearning/wav2vec/#)   
- [Conformer](https://github.com/sooftware/conformer)  
- [Emformer: RNNT Model](https://github.com/pytorch/audio/tree/main/examples/asr/emformer_rnnt)  
- [Emformer Paper](https://arxiv.org/abs/2010.10759)  
- [Nextformer](https://github.com/tuanio/nextformer)  
- [Keras based Training a CTC-based model for ASR](https://keras.io/examples/audio/ctc_asr/)   
- [alphacephei: citrinet](https://alphacephei.com/nsh/2021/04/23/citrinet.html)  
- [Coqui-ai STT](https://github.com/coqui-ai/STT)  
- [vosk Framework](https://alphacephei.com/vosk/)  
- [vosk Framework GitHub](https://github.com/alphacep/vosk-api)  
- [fairseq](https://github.com/pytorch/fairseq)   
- [TensorFlowASR](https://github.com/TensorSpeech/TensorFlowASR) [Good]    
- [Assembly AI ASR api](https://www.assemblyai.com/)  
- [Assembly AI: Building an End-to-End Speech Recognition Model in PyTorch](https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch/) [Great]  
- [BigSSL: Large-Scale Semi-Supervised Learning for Automatic Speech Recognition](https://arxiv.org/pdf/2109.13226.pdf)  
- [Tencent AI Lab: 3M-ASR](https://github.com/tencent-ailab/3m-asr)  
- [wav2seq](https://github.com/asappresearch/wav2seq)  
- [WavPrompt: speech understanding that leveraging the few-shot learning](https://github.com/Hertin/WavPrompt)  
- [Recent Advances in End-to-End Automatic Speech Recognition](https://www.nowpublishers.com/article/Details/SIP-2021-0050) [Interesting Survey]  
- [SpeechT5](https://github.com/microsoft/SpeechT5) [Interesting]  
- [TransFusion: Transcribing Speech with Multinomial Diffusion](https://github.com/RF5/transfusion-asr)  
- [Alibaba FunASR](https://github.com/alibaba-damo-academy/FunASR)  
- [Openai whisper ASR Model](https://github.com/openai/whisper) [Interesting]  
- [Openai whisper ASR Model Blog](https://openai.com/blog/whisper/)  
- [Explanation of OpenAI whisper ASR Model](https://www.linkedin.com/posts/aladdin-persson-a95384153_a-couple-of-months-ago-openai-released-its-activity-7009429017552375808-6zCI/?utm_source=share&utm_medium=member_android)  
- [High-performance inference of Whisper ASR Model](https://github.com/Const-me/Whisper)  
- [Insanely fast whisper (very fast whisper)](https://www.linkedin.com/posts/liorsinclair_you-can-now-transcribe-25-hours-of-audio-activity-7136072189933408256-G3mL?utm_source=share&utm_medium=member_desktop)  
- [Google Universal Speech Model (USM)](https://ai.googleblog.com/2023/03/universal-speech-model-usm-state-of-art.html?m=1)  
- [FaceBook Massively Multilingual Speech (MMS)](https://github.com/facebookresearch/fairseq/tree/main/examples/mms)
- [SeamlessM4T: Github](https://github.com/facebookresearch/seamless_communication)
- [SeamlessM4T: Meta AI Blog](https://ai.meta.com/blog/seamless-m4t/)
- [SeamlessM4T: Paper](https://dl.fbaipublicfiles.com/seamless/seamless_m4t_paper.pdf)  
- [SeamlessM4T: Demo](https://seamless.metademolab.com/demo)  
- [SeamlessM4T: HuggingFace Demo](https://huggingface.co/spaces/facebook/seamless_m4t)
- [SeamlessM4T v2](https://www.linkedin.com/posts/aiatmeta_today-were-sharing-the-next-milestone-in-activity-7136104235166830592-7ROv?utm_source=share&utm_medium=member_desktop)
- [WhisperFusion: Whisper + Mistral](https://github.com/collabora/WhisperFusion)
- [NeMo Canary-1B ASR Model](https://huggingface.co/spaces/nvidia/canary-1b)
- [NeMo Canary-1B Linkedin Post](https://www.linkedin.com/posts/stevehuanghe_nvidia-nemo-team-is-thrilled-to-announce-activity-7161407409448505344-HvrJ?utm_source=share&utm_medium=member_desktop)
- [Google Chirp: Universal speech model (USM)](https://cloud.google.com/speech-to-text/v2/docs/chirp-model) [Great]
- [Whisper V3 Turbo Model Linkedin Post](https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_openai-has-released-new-whisper-models-activity-7246765913331367936-NP3N?utm_source=share&utm_medium=member_desktop)
- [Gooya v1 Persian ASR Model](https://www.linkedin.com/posts/vargha-khallokhi-20b98316b_gooya-v1-persian-speech-recognition-a-hugging-activity-7258536721343016960-yfzN?utm_source=share&utm_medium=member_desktop)  
- [HuggingFace Open ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)  

## AudioLLM:
- [Qwen-Audio](https://github.com/QwenLM/Qwen-Audio)
- [Qwen2-Audio](https://github.com/QwenLM/Qwen2-Audio)
- [Qwen2-Audio Blog](https://qwenlm.github.io/blog/qwen2-audio/)  

## Speech to Speech Models:
- [Speech To Speech: an effort for an open-sourced and modular GPT4-o](https://github.com/huggingface/speech-to-speech)
- [Huggingface Multilingual Speech to Speech Library](https://www.linkedin.com/posts/andresmarafioti_introducing-hugging-faces-multilingual-activity-7236625231505731585-Pbps?utm_source=share&utm_medium=member_desktop)
- [Moshi Speech to Speech Model](https://www.linkedin.com/posts/andresmarafioti_%F0%9D%97%95%F0%9D%97%B6%F0%9D%97%B4-%F0%9D%97%BB%F0%9D%97%B2%F0%9D%98%84%F0%9D%98%80-kyutai-just-open-sourced-activity-7242210751275311104-PQEd?utm_source=share&utm_medium=member_desktop)
- [Moshi Speech to Speech Model - Link2](https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_kyutais-voice-model-moshi-is-here-moshi-activity-7242197402739253248-StiT/?utm_source=share&utm_medium=member_android)
- [Deploying Speech-to-Speech on Hugging Face](https://huggingface.co/blog/s2s_endpoint)  

### ASR with LLMs:
- [Listening with LLM](https://paul.mou.dev/posts/2023-12-31-listening-with-llm/)  

### Speech Language Modeling:
- [An Empirical Study of Language Model Integration for Transducer based Speech Recognition](https://arxiv.org/pdf/2203.16776.pdf)  
- [AudioPaLM](https://google-research.github.io/seanet/audiopalm/examples/)  

### Persian ASR Repos:
- [wav2vec2-fa](https://github.com/Hamtech-ai/wav2vec2-fa)  
- [Shenasa-ai Speech2Text](https://github.com/shenasa-ai/speech2text)  
- [SOTA Persian ASR on Common Voice](https://paperswithcode.com/sota/speech-recognition-on-common-voice-persian)  
- [Wav2Vec2 Large-xlsr Persian](https://huggingface.co/m3hrdadfi/wav2vec2-large-xlsr-persian)  
- [Wav2Vec2 Large-xlsr Persian (v3)](https://huggingface.co/m3hrdadfi/wav2vec2-large-xlsr-persian-v3)  
- [~ 200 Hours Persian ASR Data Set of Shenasa Company](https://github.com/shenasa-ai/speech2text)
- [Persian 380 Hours ASR Data Set](https://www.linkedin.com/posts/amir-pourmand_automatic-speech-recognition-farsi-youtube-activity-7198685117341454337-2sDk?utm_source=share&utm_medium=member_desktop)  

### Great Resources for Persian ASR Normalization:
- [Wav2Vec2 Large XLSR Persian v3](https://huggingface.co/m3hrdadfi/wav2vec2-large-xlsr-persian-v3)  
- [num2fawords: Convert a number into Persian word form](https://github.com/5j9/num2fawords)  
- [Parsivar: A Language Processing Toolkit for Persian](https://github.com/ICTRC/Parsivar)  
- [num2words](https://github.com/savoirfairelinux/num2words)  

### Persian based Raw Text Data Sets for LM Training:
- [naab-raw](https://huggingface.co/datasets/SLPL/naab-raw)  

### `Adapters` Method instead of fine-tuning for Large-Scale ASR models:
- [NeMo Adapters Tutorial](https://github.com/NVIDIA/NeMo/blob/main/tutorials/02_NeMo_Adapters.ipynb)  
- [Paper: Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751.pdf)  
- [Paper: Efficient Adapter Transfer of Self-Supervised Speech Models for Automatic Speech Recognition](https://arxiv.org/pdf/2202.03218.pdf)  
- [Paper: Exploiting Adapters for Cross-lingual Low-resource Speech Recognition](https://arxiv.org/pdf/2105.11905.pdf)  
- [Paper: Tiny-Attention Adapter: Contexts Are More Important Than the Number of Parameters](https://arxiv.org/abs/2211.01979)  

## Diffusion based Methods:
- [Full-band General Audio Synthesis With Score-based Diffusion](https://diffusionaudiosynthesis.github.io/)  

## Audio Generation:
- [Unconditional Audio Generation Benchmark](https://github.com/gzhu06/Unconditional-Audio-Generation-Benchmark)
- [Realtime Voice Changer](https://github.com/w-okada/voice-changer/tree/master)  

## Speech Translation:
- [Facebook XLS-R-2B-22-16 Demo](https://huggingface.co/spaces/facebook/XLS-R-2B-22-16)  
- [Facebook wav2vec2-xls-r-2b-22-to-16](https://huggingface.co/facebook/wav2vec2-xls-r-2b-22-to-16)  
- [Facebook XLS-R-2B-22-16 app code](https://huggingface.co/spaces/facebook/XLS-R-2B-22-16/blob/main/app.py)  

## G2P (Grapheme2Phoneme):
- [English Grapheme To Phoneme (G2P) Conversion](https://github.com/Kyubyong/g2p)   
- [Phonemizer: Simple text to phones converter for multiple languages](https://github.com/bootphon/phonemizer)   
- [Epitran: tool for transcribing orthographic text as IPA](https://github.com/dmort27/epitran)   
- [PersianG2P](https://github.com/PasaOpasen/PersianG2P)  
- [Persian_G2P - link2](https://github.com/AzamRabiee/Persian_G2P)   
- [Persian Attention Based G2P](https://github.com/hajix/G2P#attention-based-grapheme-to-phoneme)  
- [Tihu Dictionary for Persian Language](https://github.com/tihu-nlp/tihudict)  
- [CharsiuG2P: Multilingual G2P in over 100 languages](https://github.com/lingjzhu/CharsiuG2P)  
- [Transphone: zero-shot learning based grapheme-to-phoneme model for 8k languages](https://github.com/xinjli/transphone)  

## Fundamental Notes in Speech Processing & Courses:
- [Deep Learning for Audio (DLA)](https://github.com/markovka17/dla) [**Great Course**]  
- [MFCC Tutorial](http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/)  
- [Sequence Modeling With CTC](https://distill.pub/2017/ctc/)  
- [Explanation of Connectionist Temporal Classification](https://sid2697.github.io/Blog_Sid/algorithm/2019/10/19/CTC-Loss.html)  
- [D2L Beam Search](https://d2l.ai/chapter_recurrent-modern/beam-search.html)  
- [D2L Attention Mechanisms](https://d2l.ai/chapter_attention-mechanisms/index.html)  
- [Introduction to Speech Processing](https://wiki.aalto.fi/display/ITSP/Introduction+to+Speech+Processing) [Good]  
- [Audio Signal Proessing for Machine Learning](https://github.com/musikalkemist/AudioSignalProcessingForML)  
- [Deep Learning For Audio With Python](https://github.com/musikalkemist/DeepLearningForAudioWithPython)  
- [Deep learning (audio) application: From design to deployment](https://github.com/musikalkemist/Deep-Learning-Audio-Application-From-Design-to-Deployment)  
- [ASR 2022](https://github.com/besacier/ASR2022)
- [Hugging Face Audio course](https://huggingface.co/learn/audio-course/chapter0/introduction)  

## Great Kaldi Tutorials:
- [Kaldi Install for Dummies](https://www.assemblyai.com/blog/kaldi-install-for-dummies/)  
- [Kaldi Speech Recognition for Beginners a Simple Tutorial](https://www.assemblyai.com/blog/kaldi-speech-recognition-for-beginners-a-simple-tutorial/)  
- [Tutorial on Kaldi for Brandeis ASR course](https://github.com/keighrim/kaldi-yesno-tutorial)  

## ASR Error Correction:
- [FastCorrect](https://arxiv.org/pdf/2105.03842.pdf)  

## Source Separation:
- [Deezer source separation library](https://github.com/deezer/spleeter) [Great]   
- [Music Source Separation Challenge](https://github.com/haoheliu/2021-ISMIR-MSS-Challenge-CWS-PResUNet)  

## Sound & Audio Classification:
- [Soxan: Wav2Vec2 for speech recognition](https://github.com/m3hrdadfi/soxan)  

## Voice Activity Detection (VAD) & Speech Activity Detection (SAD):
- [Silero VAD](https://github.com/snakers4/silero-vad)  
- [Voice Activity Detection: Identifying whether someone is speaking or not](https://maelfabien.github.io/project/Speech_proj/#) [**Great**]    
- [py-webrtcvad: Python WebRTC Voice Activity Detector (VAD)](https://github.com/wiseman/py-webrtcvad) [also, it seems that it can segment audio files]   
- [Pyannote Audio](https://github.com/pyannote/pyannote-audio)  
- [Remsi: Remove silence from video files via ffmpeg](https://github.com/bambax/Remsi)  

## Audio Segmentation:  
- [inaSpeechSegmenter](https://github.com/ina-foss/inaSpeechSegmenter)  

## Extract & Remove Vocals from Song in Audio Files:
- [Moises: Remove or isolate vocals and instruments in any song](https://moises.ai/)  

## Audio Summarization:  
- [Audio Summarization API](https://github.com/ifrankandrade/api)  

## Spoken Language Recognition:
- [Spoken Language Recognition based on Kaldi](https://github.com/igorsitdikov/lid_kaldi)  

## Keyword Spotting & Speech Command Recognition:
- [PyTorch based toolkit for speech command recognition](https://github.com/idiap/sparch)  
- [Multilingual Few-Shot Keyword Spotting in PyTorch](https://github.com/alefiury/multilingual_kws_pytorch)  

## Active Learning in ASR:
- [Active learning in speech recognition](https://alphacephei.com/nsh/2021/07/13/active-learning.html)   

## Audio Pretraining, Representation Learning, and Self-Supervised Pretraining:
- [CLAP: Contrastive Language-Audio Pretraining](https://github.com/LAION-AI/CLAP)  

## Audio Augmentation:
- [Audiomentations: Audio Data Augmentation](https://github.com/iver56/audiomentations)  

## Speech Emotion Recognition:
- [Speech Emotion Recognition via wav2vec2](https://github.com/audeering/w2v2-how-to)  
- [Speech Emotions Recognition with Convolutional Neural Networks](https://www.analyticsvidhya.com/blog/2021/07/speech-emotions-recognition-with-convolutional-neural-networks/)  
- [ShEMO Data Set](https://github.com/mansourehk/ShEMO)  

## Annotation Tools:
- [audino: open source audio annotation tool](https://github.com/midas-research/audino)   
- [ASR Corpus Creator](https://github.com/egorsmkv/asr-corpus-creator)  
- [Speech Editing Toolkit](https://github.com/Zain-Jiang/Speech-Editing-Toolkit)  

## Audio Compression:
- [EnCodec: High Fidelity Neural Audio Compression](https://github.com/facebookresearch/encodec)  
- [AI Powered Audio Compression Technique](https://ai.facebook.com/blog/ai-powered-audio-compression-technique/)  

## Audio Variational Autoencoder (VAE):
- [AudioVAE: VAE implementation for audio data](https://github.com/nikuson/AudioVAE)  

## Speaker Anonymization:
- [Speaker Anonymization](https://github.com/DigitalPhonetics/speaker-anonymization)  

## Some ASR & Speech Datasets:
- [Peoples Speech](https://mlcommons.org/en/peoples-speech/)  
- [Multilingual Spoken Words](https://mlcommons.org/en/multilingual-spoken-words/)  
- [PodcastMix: A dataset for separating music and speech in podcasts](https://github.com/MTG/Podcastmix)  
- [Quran Speech to Text Dataset](https://www.openslr.org/132/)  
- [WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset](https://github.com/xinhaomei/wavcaps)
- [YODAS dataset: massive youtube speech dataset with 370k hours across 140 languages (about 100TB)](https://huggingface.co/datasets/espnet/yodas)  

## Voice Conversion:
- [Seed-VC: zero-shot voice conversion](https://github.com/Plachtaa/seed-vc)  

# Interesting Ideas about Startups with ASR:
It is interesting how quickly people implement ideas. Like the one of `podcast transcript` with Whisper. Here is a selection:  
- [podscript](https://podscript.ai/)  
- [podtext](https://podtext.ai/)  
- [podscription](https://podscription.app/)  
- [podsearch](https://podsearch.page/)  
- [Some Discussion Notes about above links](https://news.ycombinator.com/item?id=34727695)
- [Vapi: Voice AI for any application](https://vapi.ai/) [Great]  

# Other:
- [Neural Target Speech Extraction (TSE)](https://butspeechfit.github.io/tse_tutorial/)  
- [Audio Self-supervised Learning: A Survey](https://arxiv.org/abs/2203.01205)  
- [AI Audio Startups](https://github.com/csteinmetz1/ai-audio-startups)  
- [Facestar: High quality audio-visual recordings of human conversational speech](https://github.com/facebookresearch/facestar)  
- [Fast Infinite Waveform Music Generation](https://github.com/marcoppasini/musika)  
- [Nvidia Speech AI Summit 2022](https://www.nvidia.com/en-us/events/speech-ai-summit/)  
- [Poly AI](https://poly.ai/) [Interesting Company]  
- [uberduck: Open Source Voice AI Community](https://uberduck.ai/)
- [How To Build An AI Customer Service Bot](https://www.youtube.com/watch?v=87ZX56RSamA)
- [podcastfy: Open Source API alternative to NotebookLM's podcast](https://github.com/souzatharsis/podcastfy)  
